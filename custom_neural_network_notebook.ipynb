{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Two distinctive features of this particular code is that 1) you can vary the number of layers and output neurons and 2) the training samples are completely vectorised.\n",
    "\n",
    "This notebook will go through how the n-layer, m-output (n and m are varying integers) neural network works. This is a fully vectorised network, meaning that the use of for-loops is replaced with numpy vectors whenever possible to reduce the run-time.\n",
    "\n",
    "First let's visualise the digits that need to be classied. Below is an example of a \"9\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB4AAAAcCAAAAABTk1B1AAAAe0lEQVQoFX3BAU7DQAAEMe//Hz0UJSlUlc5eTpaT5WQ5WU6Wk+Uxv/LfcpsY+We5TRj5s9wmLyNvy22EIY/lNvIy8lgeIwy5LY8hhtyWtyFr5LL8mZeMXJZvk8vybXJZvk0uy4eFyWX5MLHclk9DHsvJcrKcLCfLyXLyA2+AOAGxLq7CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='training_data/image3.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first thing we need to do is to read the image files into a variable. Then we need to flatten the image data so that it is 1-dimensional, and then divide by 255 so only 1's and 0's remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "data_files = [file for file in sorted(glob.glob(\"training_data/*.png\"))] \n",
    "# sort the files based on file name\n",
    "\n",
    "data_files = sorted(data_files, key=lambda name: int(name[19:-4]))\n",
    "\n",
    "# combine RGB channels to form black-and-white image\n",
    "data = [np.asarray(cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2GRAY)) for file in data_files]\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    data[i] = np.asarray(data[i], dtype=np.uint32).flatten() / 255\n",
    "\n",
    "data = np.asarray(data)\n",
    "data = np.asarray((data.T - np.mean(data.T))/(np.std(data.T))).T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now the corresponding labels need to be read too. However as you can see the sample is very imbalanced, with many more 2's than other digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 count:  33\n",
      "1 count:  114\n",
      "2 count:  55\n",
      "3 count:  34\n",
      "4 count:  23\n",
      "5 count:  33\n",
      "6 count:  28\n",
      "7 count:  18\n",
      "8 count:  34\n",
      "9 count:  28\n"
     ]
    }
   ],
   "source": [
    "y_data1 = [1,0,9,1,1,0,1,8,3,5,1,5,3,1,6,8,7,1,8,0,1,1,9,8,1,7,6,9,0,2,0,3,2,2,0,9,4,2,7,2,0,3,7,1,8,0,1,6,3,2,1,5,4,1,4,2,1,0,2,1,6,9,6,1,2,2,1,9,1,3,5,8,1,7,2,2,1,5,2,0,4,3,1,3,2,3,9,4,5,1,7,1,3,6,3,6,1,2,4,2,0,5,2,2,1,1,7,1,1,5,6,1,8,8,2,2,1,1,5,6,1,2,4,1,3,7,1,8,6,4,2,5,7,5,8,1,9,2,0,6,2,1,3,6,1,5,3,1,0,5,5,1,2,1,2,1,8,6,1,8,2,1,0,4,1,0,3,1,3,1,2,1,1,2,4,3,8,4,1,9,2,1,9,0,1,8,8,1,0,9,1,0,0,5,6,1,5,2,2,0,1,5,3,5,3,6,2,8,1,1,3,1,1,6,1,3,8,1,2,9,1,5,2,1,1,1,2,1,9,5,2,1,8,9,1,0,3,5,4,9,1,5,2,1,5,9,2,0,9,1,7,1,2,4,1,5,7,1,8,0,3,5,1,8,8,2,1,2,1,2,6,8,9,1,3,1,5,8,1,4,6,1,4,8,7,3,4,8,3,1,1,8,9,1,4,2,1,3,6,1,6,6,9,1,2,4,1,9,9,9,2,1,0,0,1,4,0,1,4,9,1,6,1,4,5,7,1,1,3,4,3,8,2,1,5,1,0,3,1,4,9,7,7,1,2,0,1,2,5,6,6,8,2,3,1,0,9,1,1,6,8,5,1,9,5,7,7,1,8,1,1,0,8,1,2,8,1,2,8,6,2,1,9,7,1,8,3,3,6,1,1,0,5,5,2,0,0,6,3,2]\n",
    "print(\"0 count: \", y_data1.count(0))\n",
    "print(\"1 count: \", y_data1.count(1))\n",
    "print(\"2 count: \", y_data1.count(2))\n",
    "print(\"3 count: \", y_data1.count(3))\n",
    "print(\"4 count: \", y_data1.count(4))\n",
    "print(\"5 count: \", y_data1.count(5))\n",
    "print(\"6 count: \", y_data1.count(6))\n",
    "print(\"7 count: \", y_data1.count(7))\n",
    "print(\"8 count: \", y_data1.count(8))\n",
    "print(\"9 count: \", y_data1.count(9))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before moving on to the training of the network, the data needs to be balanced. However we must make sure that the order of the images and their corresponding labels are maintained when doing so, which makes it a little more tricky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new count:  99\n",
      "1 new count:  114\n",
      "2 new count:  110\n",
      "3 new count:  102\n",
      "4 new count:  115\n",
      "5 new count:  99\n",
      "6 new count:  112\n",
      "7 new count:  108\n",
      "8 new count:  102\n",
      "9 new count:  112\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "reorder = zip(y_data1, data)\n",
    "reorder = sorted(reorder, key=lambda k: k[0])\n",
    "head = []\n",
    "for i in range(0,10):\n",
    "\thead.append([])\n",
    "for item in reorder:\n",
    "\tindex = item[0]\n",
    "\thead[index].append(item)\n",
    "\n",
    "back = list(zip(*reorder))\n",
    "y_data1 = back[0]\n",
    "data = back[1]\n",
    "count = [y_data1.count(i)/len(y_data1) for i in range(0,10)]\n",
    "maxi = max(count)\n",
    "multiple = [maxi/count[i] for i in range(0,10)]\n",
    "\n",
    "for i in range(len(multiple)):\n",
    "\tmult = multiple[i]\n",
    "\tmult = int(round(mult))-1\n",
    "\tcop = copy.deepcopy(head[i])\n",
    "\tfor j in range(mult):\n",
    "\t\ttemp = cop\n",
    "\t\tfor item in temp:\n",
    "\t\t\thead[i].append(item)\n",
    "i = 0\n",
    "for line in head:\n",
    "    print(str(i) + \" new count: \", len(line))\n",
    "    i += 1\n",
    "    \n",
    "whole = []\n",
    "for i in range(0,10):\n",
    "\tfor j in range(len(head[i])):\n",
    "\t\twhole.append(head[i][j])\n",
    "\n",
    "y_data1 = list(zip(*whole))[0]\n",
    "data = list(zip(*whole))[1]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we shuffle the data (in case we only want to train with a subset of the sample) - again, the zip function is here to make sure that the images and their corresponding labels don't become mixed up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "1073\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(y_data1, data))\n",
    "random.shuffle(c)\n",
    "y_data1, data = zip(*c)\n",
    "\n",
    "batch_size = 300\n",
    "\n",
    "y_data = y_data1[:batch_size]\n",
    "print(len(y_data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Perform one-hot encoding on y_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_data = np_utils.to_categorical(y_data)\n",
    "\n",
    "Y = []\n",
    "# since one hot encoding has been performed on y_data it is useful to keep the labels of the y data in a separate array\n",
    "y_labels = []\n",
    "for y in y_data:\n",
    "    Y.append([y])\n",
    "    y_labels.append(list(y).index(1))\n",
    "Y = np.asarray(Y).squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training data and labels are now ready to be input into the network for training. From the input shape below we see that there are 400 training samples, each sample being a 840-sized numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (300, 840)\n"
     ]
    }
   ],
   "source": [
    "input_vec = data[:batch_size]\n",
    "print(\"Input shape: \", np.asarray(input_vec).shape)\n",
    "\n",
    "training_samples = len(input_vec)\n",
    "dimension = len(input_vec[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below are the functions and classes required in order to run \"forward propagate\" and \"back propagate\" later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weights:\n",
    "\tdef __init__(self, dimension, length):\n",
    "\t\tself.dimension = dimension\n",
    "\t\tself.length = length\n",
    "\t\t\n",
    "\tdef create(self):\n",
    "\t\tself.weights = np.random.uniform(-0.01,0.01,(self.dimension, self.length))\n",
    "\t\t#self.weights = np.random.rand(self.dimension, self.length)\n",
    "\t\treturn self.weights\n",
    "\n",
    "class Bias:\n",
    "\tdef __init__(self, length):\n",
    "\t\tself.length = length\n",
    "\tdef create(self):\n",
    "\t\treturn np.random.uniform(-0.001, 0.001, (self.length, 1))\n",
    "\n",
    "def initialise_weights(layers, output=10):\n",
    "\t# Includes output layer i.e. single neuron \n",
    "\tweights = []\n",
    "\tbias = []\n",
    "\tfor i in range(layers):\n",
    "\t\tif i == layers-1:\n",
    "\t\t\tlayer = Weights(output, dimension+10).create()\n",
    "\t\t\tb = Bias(output).create()\n",
    "\t\telif i == 0:\n",
    "\t\t\tlayer = Weights(dimension+10, dimension).create()\n",
    "\t\t\tb = Bias(dimension+10).create()\n",
    "\t\telse:\n",
    "\t\t\tlayer = Weights(dimension+10, dimension+10).create()\n",
    "\t\t\tb = Bias(dimension+10).create()\n",
    "\t\tweights.append(layer)\n",
    "\t\tbias.append(b)\n",
    "\n",
    "\treturn np.asarray(weights), np.asarray(bias)\n",
    "\n",
    "def relu(x):\n",
    "\tif x >= 0:\n",
    "\t\treturn x\n",
    "\telse:\n",
    "\t\treturn 0.1*x\n",
    "\n",
    "def relu_gradient(x):\n",
    "\tif x >= 0:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0.1\n",
    "\n",
    "def sigmoid(x):\n",
    "\tx = np.clip(x, -100, 100)\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "\treturn sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def loss(y, a):\n",
    "\t#return np.sum(y*np.log(a)+(1-y)*np.log(a-1))/len(y)\n",
    "\treturn np.sum(np.square(y-a))/len(y)\n",
    "\n",
    "def softmax(x):\n",
    "\tlarge = np.asarray([[item.max() for item in x]]).T \n",
    "\tx = x - large\n",
    "\ttotal = np.sum(np.exp(x), axis=1).reshape(batch_size,1)\n",
    "\treturn np.exp(x) / total\n",
    "\n",
    "def softmax_gradient(x):\n",
    "\tM = []\n",
    "\tQ = softmax(x)\n",
    "\tfor k in range(len(x)):\n",
    "\t\tp = Q[k]\n",
    "\t\tjacobian = np.diag(p*(1-p))\n",
    "\t\tfor i in range(len(jacobian)):\n",
    "\t\t\tfor j in range(len(jacobian)):\n",
    "\t\t\t\tif i == j:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tjacobian[i][j] = -p[i]*p[j]\n",
    "\t\tM.append(jacobian)\n",
    "\treturn np.asarray(M)\n",
    "\n",
    "def cross_entropy_loss(Y, A):\n",
    "\treturn np.sum(-np.log(A)*Y)/len(A)\n",
    "\n",
    "def optimize(weights, dw, bias, db, step_size=0.0001):\n",
    "\tdw = np.asarray(dw)\n",
    "\tdb = np.asarray(db)\n",
    "\tweights -= step_size*dw\n",
    "\tbias -= step_size*db\n",
    "\treturn weights, bias"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now forward_propagate. Again, this code is completely vectorized - the only place where a for-loop is used is when data is passed from one layer to the next. The values are multiplied together using the dot product, rather than a loop that multiplies and adds the values up one neuron at a time.\n",
    "\n",
    "This code is specially designed so that the number of layers and the number of output neurons can be varied.\n",
    "\n",
    "The leaky-relu function is used as the activation function because the \"relu\" part solves the vanishing gradient problem that is common for sigmoid functions and the \"leaky\" part solves the zero-gradient problem that is common in relu functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(x, weights, bias):\n",
    "\tx = np.asarray(x)\n",
    "\t\n",
    "\tZ = []\n",
    "\tA = []\n",
    "\n",
    "\tfor i in range(len(weights)):\n",
    "\t\tif i == 0:\n",
    "\t\t\tz = np.dot(x, weights[i].T) + bias[i].T \n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\tz = np.dot(A[i-1], weights[i].T) + bias[i].T\n",
    "\t\tif i == len(weights)-1:\n",
    "\t\t\ta = softmax(z)\n",
    "\t\telse:\n",
    "\t\t\tactivate = np.vectorize(relu)\n",
    "\t\t\ta = activate(z)\n",
    "\t\tZ.append(z)\n",
    "\t\tA.append(a)\n",
    "\t\n",
    "\treturn (Z, A, weights, bias)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similarly backpropagation must be designed so that it works for m > 1 number of hidden-layers and n > 0 number of output neurons.\n",
    "\n",
    "Backpropagate here uses the cross-entropy loss as the loss function, which is the default function for softmax classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(X, Z, A, weights, bias, Y):\n",
    "    # in case there is a zero\n",
    "\tA = [np.where(A[i]==0, 1e-15,A[i]) for i in range(len(A))]\n",
    "\terror = cross_entropy_loss(Y, A[-1])\n",
    "\tterm = (Y/-A[-1])\n",
    "\tDW = []\n",
    "\tDB = []\n",
    "\tfor i in range(len(weights)-1,-1,-1):\n",
    "\t\tif i == 0:\n",
    "\t\t\tdz = np.asarray(X)\n",
    "\t\t\tf = np.ones((batch_size,1))\n",
    "\t\telse:\n",
    "\t\t\tdz = A[i-1]\n",
    "\t\t\tf = np.ones((batch_size,1))\n",
    "\t\tif i == len(weights)-1:\n",
    "\t\t\tda = softmax_gradient(Z[-1])\n",
    "\t\t\tterm = np.asarray([np.dot(term[i], da[i]) for i in range(len(da))])\n",
    "\t\t\tdw = np.dot(term.T, dz)\n",
    "\t\t\tdb = np.dot(term.T, f)\n",
    "\t\telse:\n",
    "\t\t\tgradient = np.vectorize(relu_gradient)\n",
    "\t\t\tda = gradient(Z[i])\n",
    "\t\t\tdw = np.dot((term* da).T, (dz))\n",
    "\t\t\tdb = np.dot((term*da).T, (f))\n",
    "\t\t\tterm = term*da\n",
    "\n",
    "\t\tDW.append(dw)\n",
    "\t\tDB.append(db)\n",
    "\t\tdzda = weights[i] \n",
    "\t\tterm = np.dot(term, dzda)\n",
    "\n",
    "\treturn list(reversed(DW)), list(reversed(DB)), error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now the actual training. First the weights and biases are initialised to small random values. \n",
    "\n",
    "This will be a 3-layer neural network with 10 output neurons (used as input in the weight initialization). \n",
    "\n",
    "The step size will be 0.0001, and the training will run for 1000 iterations (each iteration will include all 400 training examples). A small step-size is chosen to ensure that the error curve below is smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182e009198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.0281199299081966\n"
     ]
    }
   ],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "weights, bias = initialise_weights(3, output=10)\n",
    "step = 0.0001\n",
    "\n",
    "liveplot = PlotLosses()\n",
    "\n",
    "for i in range(600):\n",
    "    \n",
    "    print(\"iteration: \", i)\n",
    "    forward_output = forward_propagate(input_vec, weights, bias)\n",
    "    Z, A, weights, bias = forward_output\n",
    "\n",
    "\n",
    "    output = [list(Z[-1][a]).index(max(list(Z[-1][a]))) for a in range(len(Z[-1]))]\n",
    "    test = y_data1[:batch_size]\n",
    "   \n",
    "    remain = np.array(output) - np.array(test)\n",
    "    \n",
    "    # values will cancel to 0 if they are the same i.e. correct\n",
    "    right = (batch_size - remain.tolist().count(0))/batch_size*100\n",
    "    print(\"Wrong: \" + str(right) + \"%\")\n",
    "\n",
    "    dw, db, error = back_propagate(input_vec, Z, A, weights, bias, Y)\n",
    "    \n",
    "    liveplot.update({\n",
    "        'cross-entropy loss': error\n",
    "    })\n",
    "    liveplot.draw()\n",
    "    \n",
    "    print(\"error: \", error)\n",
    "\n",
    "    new_weights, new_bias = optimize(weights, dw, bias, db, step_size=step)\n",
    "    weights = new_weights\n",
    "    bias = new_bias\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As you can see the error function converges and settles at a loss of 0.03 from >2.0. This neural network is not 100% accurate but there are many improvements that can be made, such as including drop-out weights and using ridge-regression to prevent over-fitting. Using convolutions/kernals for feature learning will also improve the accuracy. \n",
    "\n",
    "The weights and biases can be written to a json file so it can be reused later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
